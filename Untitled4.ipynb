{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82c1147e-652c-4f13-bdef-b2dadc6eb7fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10240 files belonging to 4 classes.\n",
      "Found 1283 files belonging to 4 classes.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter client ID (0 or 1):  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: flwr.client.start_numpy_client() is deprecated. \n",
      "\tInstead, use `flwr.client.start_client()` by ensuring you first call the `.to_client()` method as shown below: \n",
      "\tflwr.client.start_client(\n",
      "\t\tserver_address='<IP>:<PORT>',\n",
      "\t\tclient=FlowerClient().to_client(), # <-- where FlowerClient is of type flwr.client.NumPyClient object\n",
      "\t)\n",
      "\tUsing `start_numpy_client()` is deprecated.\n",
      "\n",
      "            This is a deprecated feature. It will be removed\n",
      "            entirely in future versions of Flower.\n",
      "        \n",
      "\u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: flwr.client.start_client() is deprecated.\n",
      "\tInstead, use the `flower-supernode` CLI command to start a SuperNode as shown below:\n",
      "\n",
      "\t\t$ flower-supernode --insecure --superlink='<IP>:<PORT>'\n",
      "\n",
      "\tTo view all available options, run:\n",
      "\n",
      "\t\t$ flower-supernode --help\n",
      "\n",
      "\tUsing `start_client()` is deprecated.\n",
      "\n",
      "            This is a deprecated feature. It will be removed\n",
      "            entirely in future versions of Flower.\n",
      "        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Starting client 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      Received: train message 84064cb8-f0dd-4719-8d4f-412d83f1c541\n",
      "\u001b[92mINFO \u001b[0m:      Sent reply\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      Received: evaluate message e17ff0fa-8427-4eb6-ab20-e1ec672b478f\n",
      "\u001b[92mINFO \u001b[0m:      Sent reply\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      Received: train message 2f068893-c28d-40ac-8c08-bcb45a2509f3\n",
      "\u001b[92mINFO \u001b[0m:      Sent reply\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      Received: evaluate message f665a17e-99a9-4daa-9833-0c01838e0af9\n",
      "\u001b[92mINFO \u001b[0m:      Sent reply\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      Received: train message 60c6adce-12dc-4b01-b45c-72cd9e5e1c58\n",
      "\u001b[92mINFO \u001b[0m:      Sent reply\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      Received: evaluate message 02d0df4f-ef9c-48d7-a309-eef96a18f3b6\n",
      "\u001b[92mINFO \u001b[0m:      Sent reply\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      Received: reconnect message 557c01b7-4340-4272-befb-0c8c755b1586\n",
      "\u001b[92mINFO \u001b[0m:      Disconnect and shut down\n"
     ]
    }
   ],
   "source": [
    "import flwr as fl\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# -------------------------\n",
    "#   Load and preprocess data\n",
    "# -------------------------\n",
    "img_size = (128, 128)\n",
    "batch_size = 32\n",
    "\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    \"AlzheimerDataset/train\",\n",
    "    image_size=img_size,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    \"AlzheimerDataset/test\",\n",
    "    image_size=img_size,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "# Normalize images\n",
    "train_ds = train_ds.map(lambda x, y: (x/255.0, y))\n",
    "test_ds = test_ds.map(lambda x, y: (x/255.0, y))\n",
    "\n",
    "# Partition data across clients\n",
    "def partition_dataset(dataset, client_id, num_clients=2):\n",
    "    dataset = dataset.shuffle(10000)\n",
    "    total_size = len(dataset)\n",
    "    part_size = total_size // num_clients\n",
    "    start = client_id * part_size\n",
    "    end = start + part_size\n",
    "    return dataset.skip(start).take(part_size)\n",
    "\n",
    "client_id = int(input(\"Enter client ID (0 or 1): \"))\n",
    "train_client_ds = partition_dataset(train_ds, client_id, num_clients=2)\n",
    "\n",
    "# Convert partitioned dataset to numpy arrays for computing class weights\n",
    "x_train_list, y_train_list = [], []\n",
    "for batch_x, batch_y in train_client_ds:\n",
    "    x_train_list.append(batch_x.numpy())\n",
    "    y_train_list.append(batch_y.numpy())\n",
    "\n",
    "x_train_client = np.concatenate(x_train_list)\n",
    "y_train_client = np.concatenate(y_train_list)\n",
    "\n",
    "# Compute class weights\n",
    "classes = np.unique(y_train_client)\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=classes,\n",
    "    y=y_train_client\n",
    ")\n",
    "class_weights = dict(zip(classes, class_weights))\n",
    "\n",
    "# -------------------------\n",
    "#   Define CNN model\n",
    "# -------------------------\n",
    "def create_cnn_model():\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=img_size + (3,)),\n",
    "        layers.Conv2D(32, (3,3), activation='relu'),\n",
    "        layers.MaxPooling2D(),\n",
    "        layers.Conv2D(64, (3,3), activation='relu'),\n",
    "        layers.MaxPooling2D(),\n",
    "        layers.Conv2D(128, (3,3), activation='relu'),\n",
    "        layers.MaxPooling2D(),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dense(4, activation='softmax')\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "model = create_cnn_model()\n",
    "\n",
    "# -------------------------\n",
    "#   Flower client class\n",
    "# -------------------------\n",
    "class AlzheimerClient(fl.client.NumPyClient):\n",
    "    def get_parameters(self, config=None):\n",
    "        return model.get_weights()\n",
    "\n",
    "    def fit(self, parameters, config):\n",
    "        model.set_weights(parameters)\n",
    "        model.fit(\n",
    "            x_train_client, y_train_client,\n",
    "            epochs=1,\n",
    "            batch_size=32,\n",
    "            class_weight=class_weights,   # âœ… Apply weights\n",
    "            verbose=0\n",
    "        )\n",
    "        return model.get_weights(), len(x_train_client), {}\n",
    "\n",
    "    def evaluate(self, parameters, config=None):\n",
    "        model.set_weights(parameters)\n",
    "\n",
    "        # Evaluate overall loss/accuracy\n",
    "        loss, acc = model.evaluate(test_ds, verbose=0)\n",
    "\n",
    "        # Get predictions for per-class accuracy\n",
    "        y_true, y_pred = [], []\n",
    "        for batch_x, batch_y in test_ds:\n",
    "            preds = model.predict(batch_x, verbose=0)\n",
    "            preds = np.argmax(preds, axis=1)\n",
    "            y_true.extend(batch_y.numpy())\n",
    "            y_pred.extend(preds)\n",
    "\n",
    "        y_true = np.array(y_true)\n",
    "        y_pred = np.array(y_pred)\n",
    "\n",
    "        # Generate per-class metrics\n",
    "        report = classification_report(\n",
    "            y_true, y_pred, output_dict=True, zero_division=0\n",
    "        )\n",
    "\n",
    "        # Extract per-class accuracy\n",
    "        class_acc = {f\"class_{cls}_acc\": report[str(cls)][\"recall\"] \n",
    "                     for cls in np.unique(y_true)}\n",
    "\n",
    "        # Merge metrics\n",
    "        metrics = {\"loss\": loss, \"accuracy\": acc}\n",
    "        metrics.update(class_acc)\n",
    "\n",
    "        return loss, len(test_ds), metrics\n",
    "\n",
    "# -------------------------\n",
    "#   Start client\n",
    "# -------------------------\n",
    "print(f\"ðŸš€ Starting client {client_id}\")\n",
    "fl.client.start_numpy_client(server_address=\"127.0.0.1:8080\", client=AlzheimerClient())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62223638-0f5f-447c-b88c-7e45ac9cee9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
